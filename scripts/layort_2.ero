Running tokenizer on dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=4):  25%|██▌       | 250/1000 [00:00<00:00, 1577.71 examples/s]                                                                                                      merge and mask:   0%|          | 0/1000 [00:00<?, ? examples/s]merge and mask: 100%|██████████| 1000/1000 [00:00<00:00, 1539.90 examples/s]                                                                            padding:   0%|          | 0/1000 [00:00<?, ? examples/s]padding: 100%|██████████| 1000/1000 [00:00<00:00, 5446.09 examples/s]                                                                       0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "sample_gcn_layort_2.py", line 290, in <module>
    main()
  File "sample_gcn_layort_2.py", line 224, in main
    samples = sample_fn(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 777, in ddim_sample_loop
    for sample in self.ddim_sample_loop_progressive(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 838, in ddim_sample_loop_progressive
    out = self.ddim_sample(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 703, in ddim_sample
    sample=langevin_fn(sample, graph,t)
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/sample_utils_layort_2.py", line 53, in langevin_fn_gsn
    sample_update = model_gsn(graph)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/sample_utils_layort_2.py", line 147, in forward
    graph[i].multi_update_all({'to':(self.gsn_msg_forward,partial(gsn_reduce_forward,self.W,self.Wx,self.Wr,1,self.n_nodes,self.n_token,self.n_token_dim,self.pad_token_id)),
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/heterograph.py", line 4909, in multi_update_all
    all_out[dtid].append(core.message_passing(g, mfunc, rfunc, afunc))
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/core.py", line 407, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/core.py", line 174, in invoke_udf_reduce
    retf.update_row(merged_nodes, merged_rst)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/frame.py", line 816, in update_row
    self._columns[key].update(rowids, val)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/frame.py", line 390, in update
    self.data = F.scatter_row(self.data, rowids, feats)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py", line 259, in scatter_row
    return data.index_copy(0, row_index.long(), value)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_index_copy)
/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 16732) of binary: /HOME/scz0b8n/.conda/envs/diffuseq/bin/python
Traceback (most recent call last):
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
sample_gcn_layort_2.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-03-29_11:12:01
  host      : g0096.para.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 16732)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running tokenizer on dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 6634.39 examples/s]                                                                                                      merge and mask:   0%|          | 0/1000 [00:00<?, ? examples/s]merge and mask: 100%|██████████| 1000/1000 [00:00<00:00, 7543.05 examples/s]                                                                            padding:   0%|          | 0/1000 [00:00<?, ? examples/s]padding: 100%|██████████| 1000/1000 [00:00<00:00, 5407.61 examples/s]                                                                       0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "sample_gcn_layort_2.py", line 290, in <module>
    main()
  File "sample_gcn_layort_2.py", line 224, in main
    samples = sample_fn(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 777, in ddim_sample_loop
    for sample in self.ddim_sample_loop_progressive(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 838, in ddim_sample_loop_progressive
    out = self.ddim_sample(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 703, in ddim_sample
    sample=langevin_fn(sample, graph,t)
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/sample_utils_layort_2.py", line 53, in langevin_fn_gsn
    sample_update = model_gsn(graph)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/sample_utils_layort_2.py", line 147, in forward
    graph[i].multi_update_all({'to':(self.gsn_msg_forward,partial(gsn_reduce_forward,self.W,self.Wx,self.Wr,1,self.n_nodes,self.n_token,self.n_token_dim,self.pad_token_id)),
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/heterograph.py", line 4909, in multi_update_all
    all_out[dtid].append(core.message_passing(g, mfunc, rfunc, afunc))
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/core.py", line 407, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/core.py", line 174, in invoke_udf_reduce
    retf.update_row(merged_nodes, merged_rst)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/frame.py", line 816, in update_row
    self._columns[key].update(rowids, val)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/frame.py", line 390, in update
    self.data = F.scatter_row(self.data, rowids, feats)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py", line 259, in scatter_row
    return data.index_copy(0, row_index.long(), value)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_index_copy)
/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 16865) of binary: /HOME/scz0b8n/.conda/envs/diffuseq/bin/python
Traceback (most recent call last):
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
sample_gcn_layort_2.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-03-29_11:12:27
  host      : g0096.para.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 16865)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running tokenizer on dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 6878.86 examples/s]                                                                                                      merge and mask:   0%|          | 0/1000 [00:00<?, ? examples/s]merge and mask: 100%|██████████| 1000/1000 [00:00<00:00, 7422.85 examples/s]                                                                            padding:   0%|          | 0/1000 [00:00<?, ? examples/s]padding: 100%|██████████| 1000/1000 [00:00<00:00, 5335.12 examples/s]                                                                       0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "sample_gcn_layort_2.py", line 290, in <module>
    main()
  File "sample_gcn_layort_2.py", line 224, in main
    samples = sample_fn(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 777, in ddim_sample_loop
    for sample in self.ddim_sample_loop_progressive(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 838, in ddim_sample_loop_progressive
    out = self.ddim_sample(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 703, in ddim_sample
    sample=langevin_fn(sample, graph,t)
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/sample_utils_layort_2.py", line 53, in langevin_fn_gsn
    sample_update = model_gsn(graph)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/sample_utils_layort_2.py", line 147, in forward
    graph[i].multi_update_all({'to':(self.gsn_msg_forward,partial(gsn_reduce_forward,self.W,self.Wx,self.Wr,1,self.n_nodes,self.n_token,self.n_token_dim,self.pad_token_id)),
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/heterograph.py", line 4909, in multi_update_all
    all_out[dtid].append(core.message_passing(g, mfunc, rfunc, afunc))
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/core.py", line 407, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/core.py", line 174, in invoke_udf_reduce
    retf.update_row(merged_nodes, merged_rst)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/frame.py", line 816, in update_row
    self._columns[key].update(rowids, val)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/frame.py", line 390, in update
    self.data = F.scatter_row(self.data, rowids, feats)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py", line 259, in scatter_row
    return data.index_copy(0, row_index.long(), value)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_index_copy)
/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 17060) of binary: /HOME/scz0b8n/.conda/envs/diffuseq/bin/python
Traceback (most recent call last):
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
sample_gcn_layort_2.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-03-29_11:12:54
  host      : g0096.para.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 17060)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running tokenizer on dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 6854.14 examples/s]                                                                                                      merge and mask:   0%|          | 0/1000 [00:00<?, ? examples/s]merge and mask: 100%|██████████| 1000/1000 [00:00<00:00, 7527.21 examples/s]                                                                            padding:   0%|          | 0/1000 [00:00<?, ? examples/s]padding: 100%|██████████| 1000/1000 [00:00<00:00, 5441.61 examples/s]                                                                       0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "sample_gcn_layort_2.py", line 290, in <module>
    main()
  File "sample_gcn_layort_2.py", line 224, in main
    samples = sample_fn(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 777, in ddim_sample_loop
    for sample in self.ddim_sample_loop_progressive(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 838, in ddim_sample_loop_progressive
    out = self.ddim_sample(
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/diffuseq/gaussian_diffusion_layort_2.py", line 703, in ddim_sample
    sample=langevin_fn(sample, graph,t)
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/sample_utils_layort_2.py", line 53, in langevin_fn_gsn
    sample_update = model_gsn(graph)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/run01/scz0b8n/xrzhao/DiffuSeq-main/sample_utils_layort_2.py", line 147, in forward
    graph[i].multi_update_all({'to':(self.gsn_msg_forward,partial(gsn_reduce_forward,self.W,self.Wx,self.Wr,1,self.n_nodes,self.n_token,self.n_token_dim,self.pad_token_id)),
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/heterograph.py", line 4909, in multi_update_all
    all_out[dtid].append(core.message_passing(g, mfunc, rfunc, afunc))
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/core.py", line 407, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/core.py", line 174, in invoke_udf_reduce
    retf.update_row(merged_nodes, merged_rst)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/frame.py", line 816, in update_row
    self._columns[key].update(rowids, val)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/frame.py", line 390, in update
    self.data = F.scatter_row(self.data, rowids, feats)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py", line 259, in scatter_row
    return data.index_copy(0, row_index.long(), value)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_index_copy)
/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 17261) of binary: /HOME/scz0b8n/.conda/envs/diffuseq/bin/python
Traceback (most recent call last):
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/HOME/scz0b8n/.conda/envs/diffuseq/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
sample_gcn_layort_2.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-03-29_11:13:20
  host      : g0096.para.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 17261)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
